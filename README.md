# ğŸš€ E-Commerce Customer Intelligence & Analytics Pipeline

> **Data Analytics Processing & Storage for customer segmentation, behavioral analytics, and retail intelligence**

[![Pipeline Status](https://img.shields.io/badge/pipeline-passing-brightgreen)]()
[![dbt Models](https://img.shields.io/badge/dbt%20models-6-blue)]()
[![Test Coverage](https://img.shields.io/badge/tests-26-success)]()
[![Data Quality](https://img.shields.io/badge/quality-100%25-brightgreen)]()

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Local Environment Setup](#local-environment-setup)
- [Running the Data Export Application](#running-the-data-export-application)
- [GitLab CI/CD Pipeline](#gitlab-cicd-pipeline)
- [Data Model Documentation](#data-model-documentation)
- [dbt Models & Business Logic](#dbt-models--business-logic)
- [Viewing dbt Documentation](#viewing-dbt-documentation)
- [Sample Queries](#sample-queries)
- [Project Structure](#project-structure)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

This project implements a modern data lakehouse architecture for e-commerce analytics, featuring:

- Customer Segmentation analysis & intelligence (with rankings,category performance metrics)
- Sales Analytics including Daily aggregations with trend analysis and forecasting
-  Automated tests ensuring data accuracy & CI/CD Automation with GitLab pipeline

---

## ğŸ—ï¸ Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Products    â”‚     â”‚   Users      â”‚     â”‚    Carts     â”‚   â”‚
â”‚  â”‚  (Fake API)  â”‚     â”‚  (Fake API)  â”‚     â”‚  (Fake API)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                    â”‚                     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                    â”‚                     â”‚
          â†“                    â†“                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BRONZE LAYER (Raw Zone)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MinIO Object Storage + PostgreSQL Raw Schema            â”‚  â”‚
â”‚  â”‚  â€¢ JSON files preserved                                  â”‚  â”‚
â”‚  â”‚  â€¢ Immutable raw data                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SILVER LAYER (Staging)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PostgreSQL Raw Schema                                   â”‚  â”‚
â”‚  â”‚  â€¢ Cleaned and typed data                                â”‚  â”‚
â”‚  â”‚  â€¢ Deduplication applied                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GOLD LAYER (Analytics)                          â”‚
â”‚                   dbt Transformations                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Dimensions   â”‚  â”‚     Facts      â”‚  â”‚   Aggregates     â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ â€¢ dim_products â”‚  â”‚ â€¢ fct_carts    â”‚  â”‚ â€¢ Daily Sales    â”‚ â”‚
â”‚  â”‚ â€¢ dim_users    â”‚  â”‚ â€¢ fct_daily    â”‚  â”‚ â€¢ Category Perf  â”‚ â”‚
â”‚  â”‚ â€¢ dim_segments â”‚  â”‚   _sales       â”‚  â”‚ â€¢ User Segments  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Kubernetes (k3d) | Container orchestration |
| **Workflow** | Apache Airflow 2.8.0(WIP) | Pipeline scheduling |
| **Transformation** | dbt  | SQL-based transformations |
| **Data Warehouse** | PostgreSQL |
| **Object Storage** | MinIO | Raw data lake |
| **CI/CD** | GitLab CI/CD | Automated deployments |
| **Data Source** | Fake Store API | Sample e-commerce data |

### Medallion Architecture

This project implements a **medallion architecture** (Bronze â†’ Silver â†’ Gold):

1. **Bronze (Raw)**: Immutable source data stored in MinIO + PostgreSQL raw schema
2. **Silver (Staging)**: Cleaned, typed, and deduplicated data
3. **Gold (Analytics)**: Business-ready dimension and fact tables

---

##  Features

### Data Quality
- **Automated Tests**: Uniqueness, not-null, relationships, accepted values

### Analytics Capabilities

- ğŸ“Š **Daily Sales Trends**: Track revenue, cart count, and customer metrics
- ğŸ›ï¸ **Category Performance**: Identify top-performing product categories
- ğŸ‘¥ **Customer Lifetime Value**: Calculate and segment by CLV
- ğŸ“ˆ **Product Rankings**: Star, A-tier, B-tier, C-tier products
- ğŸ”„ **Incremental Loading**: Efficient updates for large datasets

---

## ğŸ“¦ Prerequisites

### Required Software

| Software | Version | Installation |
|----------|---------|-------------|
| **Docker** | 20.10+ | [Install Docker](https://docs.docker.com/get-docker/) |
| **kubectl** | 1.28+ | [Install kubectl](https://kubernetes.io/docs/tasks/tools/) |
| **k3d** | 5.6+ | `curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh \| bash` |
| **Git** | 2.0+ | [Install Git](https://git-scm.com/downloads) |

### Optional Tools

- **GitLab Runner** (for local CI/CD): [Install GitLab Runner](https://docs.gitlab.com/runner/install/)
- **dbt CLI** (for local dbt runs): `pip install dbt-postgres==1.9.0`

---

##  Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/YOUR_USERNAME/ecommerce-customer-intelligence.git
cd ecommerce-customer-intelligence

# 2. Create k3d cluster
k3d cluster create pipeline --agents 2

# 3. Setup infrastructure (PostgreSQL, MinIO, secrets)
./scripts/setup.sh

# 4. Run complete pipeline
./scripts/run-complete-workflow.sh

# 5. View results
kubectl exec -n ecommerce-pipeline \
  $(kubectl get pod -n ecommerce-pipeline -l app=postgresql -o jsonpath="{.items[0].metadata.name}") \
  -- psql -U ecommerceuser -d ecommerce_dw -c "SELECT * FROM analytics.dim_user_segments LIMIT 10;"
```
![Expected output](images/Data_Sample.png)

Sample data retrieved. ğŸ‰

---

## ğŸ› ï¸ Local Environment Setup


#### 1. Create Kubernetes Cluster
```bash
# Create k3d cluster with 1 server + 2 agents
k3d cluster create pipeline --agents 2

# Verify cluster is running
kubectl cluster-info
kubectl get nodes

```

#### 2. Deploy Infrastructure

The setup script automates:
- âœ… Namespace creation
- âœ… Secret creation (PostgreSQL, MinIO)
- âœ… PostgreSQL deployment
- âœ… MinIO deployment
- âœ… Database schema initialization
- âœ… Docker image builds and imports
```bash
# Run automated setup
./scripts/setup.sh

# What it does:
# 1. Creates namespace 'ecommerce-pipeline'
# 2. Deploys PostgreSQL database
# 3. Deploys MinIO object storage
# 4. Creates secrets for credentials
# 5. Initializes database schemas (raw, analytics)
# 6. Builds Docker images (data-export, dbt)
# 7. Imports images to k3d cluster

# Verify deployment
kubectl get pods -n ecommerce-pipeline

# Expected output:
# NAME                          READY   STATUS    RESTARTS   AGE
# postgresql-xxxxxxxxxx-xxxxx   1/1     Running   0          2m
# minio-xxxxxxxxxx-xxxxx        1/1     Running   0          2m
```

#### 3. Verify Services
```bash
# Check all resources
kubectl get all -n ecommerce-pipeline

# Test PostgreSQL connection
kubectl exec -n ecommerce-pipeline \
  $(kubectl get pod -n ecommerce-pipeline -l app=postgresql -o jsonpath="{.items[0].metadata.name}") \
  -- psql -U ecommerceuser -d ecommerce_dw -c "SELECT version();"

# Test MinIO (from inside cluster)
kubectl exec -n ecommerce-pipeline \
  $(kubectl get pod -n ecommerce-pipeline -l app=minio -o jsonpath="{.items[0].metadata.name}") \
  -- mc alias set local http://localhost:9000 minioadmin MinioPass123
```

#### 4. Port Forward for Local Access (Optional)
```bash
# PostgreSQL (in one terminal)
kubectl port-forward -n ecommerce-pipeline svc/postgresql-service 5432:5432

# MinIO Console (in another terminal)
kubectl port-forward -n ecommerce-pipeline svc/minio-service 9001:9001

# Access MinIO Console at: http://localhost:9001
# Username: minioadmin
# Password: MinioPass123
```

---

## Running the Data Export Application

### Manual Execution

The data export application extracts data from Fake Store API and loads it to MinIO and PostgreSQL.
```bash
# Run data load script
./scripts/load-data.sh

# What it does:
# 1. Fetches products from https://fakestoreapi.com/products
# 2. Fetches users from https://fakestoreapi.com/users
# 3. Fetches carts from https://fakestoreapi.com/carts
# 4. Stores JSON files in MinIO (raw-data bucket)
# 5. Loads structured data to PostgreSQL raw schema

# Verify data loaded
kubectl exec -n ecommerce-pipeline \
  $(kubectl get pod -n ecommerce-pipeline -l app=postgresql -o jsonpath="{.items[0].metadata.name}") \
  -- psql -U ecommerceuser -d ecommerce_dw -c "
    SELECT 'products' as table_name, COUNT(*) as records FROM raw.products
    UNION ALL
    SELECT 'users', COUNT(*) FROM raw.users
    UNION ALL
    SELECT 'carts', COUNT(*) FROM raw.carts;
  "
```

### Kubernetes Job Execution
```bash
# Run as Kubernetes Job
kubectl create job data-export-manual \
  --image=data-export:latest \
  --namespace=ecommerce-pipeline \
  -- python /app/main.py

# Watch job progress
kubectl logs -n ecommerce-pipeline job/data-export-manual -f

# Check job status
kubectl get jobs -n ecommerce-pipeline
```

### Understanding the Data Export Application

**Source Code**: `data-export/main.py`
```python
# Key functions:
# 1. fetch_api_data()      - Fetches from Fake Store API
# 2. upload_to_minio()     - Stores raw JSON in MinIO
# 3. load_to_postgres()    - Loads to PostgreSQL raw schema
# 4. main()                - Orchestrates the ETL process
```

**Configuration**:
- API endpoint: `https://fakestoreapi.com`
- MinIO bucket: `raw-data`
- PostgreSQL schema: `raw`
- Tables: `products`, `users`, `carts`

---

## ğŸ”„ GitLab CI/CD Pipeline

### Pipeline Overview

The GitLab CI/CD pipeline automates the entire workflow with 4 stages:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: BUILD (Automatic on code changes)             â”‚
â”‚ â”œâ”€ build:data-export  â†’ Build Python extraction app    â”‚
â”‚ â””â”€ build:dbt          â†’ Build dbt transformation image â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STAGE 2: TEST (Automatic)                              â”‚
â”‚ â”œâ”€ test:dbt-parse     â†’ Validate dbt project structure â”‚
â”‚ â”œâ”€ test:dbt-compile   â†’ Compile SQL models             â”‚
â”‚ â””â”€ test:lint-yaml     â†’ Lint configuration files       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STAGE 3: DEPLOY (Manual trigger)                       â”‚
â”‚ â”œâ”€ deploy:infrastructure â†’ Deploy K8s resources        â”‚
â”‚ â””â”€ deploy:docker-images  â†’ Load images to cluster      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STAGE 4: TRANSFORM (Manual trigger)                    â”‚
â”‚ â”œâ”€ transform:load-data   â†’ Extract from API            â”‚
â”‚ â”œâ”€ transform:dbt-run     â†’ Run transformations         â”‚
â”‚ â”œâ”€ transform:dbt-test    â†’ Validate data quality       â”‚
â”‚ â””â”€ transform:dbt-docs    â†’ Generate documentation      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Configuration

**File**: `.gitlab-ci.yml`

Key features:
- âœ… **Automated builds** on code changes
- âœ… **Quality gates** with automated tests
- âœ… **Manual deployment** controls for safety
- âœ… **Artifact preservation** (Docker images, dbt docs)
- âœ… **Job cleanup** after pipeline completion

### Setting Up GitLab CI/CD

#### 1. Create GitLab Repository
```bash
# Option A: Create via GitLab UI
# 1. Go to https://gitlab.com
# 2. Click "New Project" â†’ "Create blank project"
# 3. Name: "ecommerce-customer-intelligence"
# 4. Click "Create project"

# Option B: Push existing repository
git remote add gitlab https://gitlab.com/YOUR_USERNAME/ecommerce-customer-intelligence.git
git push gitlab main
```

#### 2. Configure GitLab Runner
```bash
# Install GitLab Runner (macOS)
brew install gitlab-runner

# Install GitLab Runner (Linux)
curl -L "https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh" | sudo bash
sudo apt-get install gitlab-runner

# Register the runner
gitlab-runner register

# When prompted:
# GitLab instance URL: https://gitlab.com
# Registration token: [Get from Settings â†’ CI/CD â†’ Runners]
# Description: local-k3d-runner
# Tags: docker,kubernetes
# Executor: docker
# Default image: alpine:latest
```

#### 3. Configure Runner for Kubernetes Access
```bash
# Edit runner config
sudo nano /etc/gitlab-runner/config.toml

# Add these volumes:
[[runners]]
  [runners.docker]
    volumes = [
      "/var/run/docker.sock:/var/run/docker.sock",
      "/usr/local/bin/kubectl:/usr/local/bin/kubectl:ro",
      "$HOME/.kube:/root/.kube:ro"
    ]
```

#### 4. Set GitLab CI/CD Variables

In GitLab project: **Settings â†’ CI/CD â†’ Variables**

| Variable | Value | Masked | Protected |
|----------|-------|--------|-----------|
| `POSTGRES_PASSWORD` | `SecurePass123` | âœ… | âœ… |
| `MINIO_PASSWORD` | `MinioPass123` | âœ… | âœ… |
| `NAMESPACE` | `ecommerce-pipeline` | âŒ | âŒ |

#### 5. Trigger Pipeline
```bash
# Push to GitLab
git push gitlab main

# Pipeline automatically runs BUILD and TEST stages
# DEPLOY and TRANSFORM stages require manual trigger
```

### Viewing Pipeline

1. Go to **CI/CD â†’ Pipelines** in GitLab
2. Click on a pipeline to see all jobs

### Manual Deployment

For `deploy` and `transform` stages:

1. Go to pipeline view
2. Click on the job you want to run
3. Confirm execution
4. Monitor job logs in real-time

---

## ğŸ“Š Data Model Documentation

### Entity Relationship Diagram (ERD)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   raw.products      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ id (PK)          â”‚
â”‚ â€¢ title            â”‚
â”‚ â€¢ price            â”‚
â”‚ â€¢ category         â”‚
â”‚ â€¢ description      â”‚
â”‚ â€¢ image            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ 1:N (product appears in many carts)
           â”‚
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    raw.carts        â”‚    N:1  â”‚    raw.users        â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ id (PK)          â”‚         â”‚ â€¢ id (PK)          â”‚
â”‚ â€¢ user_id (FK) â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â€¢ email            â”‚
â”‚ â€¢ date             â”‚         â”‚ â€¢ username         â”‚
â”‚ â€¢ products (JSON)  â”‚         â”‚ â€¢ password         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â€¢ name             â”‚
           â”‚                    â”‚ â€¢ address          â”‚
           â”‚                    â”‚ â€¢ phone            â”‚
           â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â”‚ dbt transformations            â”‚
           â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analytics.fct_carts â”‚         â”‚ analytics.dim_users â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ cart_id (PK)     â”‚    N:1  â”‚ â€¢ user_id (PK)     â”‚
â”‚ â€¢ user_id (FK) â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â€¢ email            â”‚
â”‚ â€¢ cart_date        â”‚         â”‚ â€¢ full_name        â”‚
â”‚ â€¢ total_items      â”‚         â”‚ â€¢ city             â”‚
â”‚ â€¢ unique_products  â”‚         â”‚ â€¢ phone            â”‚
â”‚ â€¢ created_at       â”‚         â”‚ â€¢ created_at       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â”‚                               â”‚
           â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analytics.fct_daily_sales   â”‚ â”‚ analytics.dim_user_segments â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ sale_date (PK)           â”‚ â”‚ â€¢ user_id (PK, FK)         â”‚
â”‚ â€¢ total_carts              â”‚ â”‚ â€¢ recency_score            â”‚
â”‚ â€¢ unique_customers         â”‚ â”‚ â€¢ frequency_score          â”‚
â”‚ â€¢ total_items_sold         â”‚ â”‚ â€¢ monetary_score           â”‚
â”‚ â€¢ avg_items_per_cart       â”‚ â”‚ â€¢ rfm_score                â”‚
â”‚ â€¢ avg_carts_per_customer   â”‚ â”‚ â€¢ user_segment             â”‚
â”‚ â€¢ updated_at               â”‚ â”‚ â€¢ segment_tier             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â€¢ created_at               â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analytics.dim_products              â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ product_id (PK)                  â”‚
â”‚ â€¢ product_name                     â”‚
â”‚ â€¢ price                            â”‚
â”‚ â€¢ category                         â”‚
â”‚ â€¢ product_tier                     â”‚
â”‚ â€¢ is_premium                       â”‚
â”‚ â€¢ created_at                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analytics.fct_category_performance  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ â€¢ category (PK)                    â”‚
â”‚ â€¢ total_products                   â”‚
â”‚ â€¢ avg_price                        â”‚
â”‚ â€¢ min_price                        â”‚
â”‚ â€¢ max_price                        â”‚
â”‚ â€¢ price_range                      â”‚
â”‚ â€¢ updated_at                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Lineage
```
Fake Store API
    â”‚
    â”œâ”€â”€â”€ /products â”€â”€â”€â”€â”€â†’ raw.products â”€â”€â”€â”€â”€â†’ dim_products â”€â”€â”€â”€â”€â†’ fct_category_performance
    â”‚                                             â”‚
    â”œâ”€â”€â”€ /users â”€â”€â”€â”€â”€â”€â”€â”€â†’ raw.users â”€â”€â”€â”€â”€â”€â”€â”€â†’ dim_users â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ dim_user_segments
    â”‚                                             â”‚
    â””â”€â”€â”€ /carts â”€â”€â”€â”€â”€â”€â”€â”€â†’ raw.carts â”€â”€â”€â”€â”€â”€â”€â”€â†’ fct_carts â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ fct_daily_sales
                                                  â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (aggregations)
```

### Schema Descriptions

#### Raw Schema (`raw.*`)

**Purpose**: Immutable landing zone for source data

| Table | Records | Description |
|-------|---------|-------------|
| `raw.products` | ~20 | Product catalog from API |
| `raw.users` | ~10 | Customer profiles |
| `raw.carts` | ~20 | Shopping cart transactions |

#### Analytics Schema (`analytics.*`)

**Purpose**: Business-ready, transformed data

| Table | Type | Records | Description |
|-------|------|---------|-------------|
| `dim_products` | Dimension | ~20 | Product master data with tiers |
| `dim_users` | Dimension | ~10 | Customer profiles cleaned |
| `dim_user_segments` | Dimension | ~10 | RFM customer segmentation |
| `fct_carts` | Fact | ~20 | Cart transactions grain: 1 row per cart |
| `fct_daily_sales` | Fact | ~varies | Daily sales metrics (incremental) |
| `fct_category_performance` | Aggregate | ~4 | Category-level KPIs |

---

## ğŸ¨ dbt Models & Business Logic

### Model Hierarchy
```
models/
â”œâ”€â”€ staging/
â”‚   â””â”€â”€ (future: stg_* models for cleaning)
â”œâ”€â”€ dimensions/
â”‚   â”œâ”€â”€ dim_products.sql          â­ Product master
â”‚   â”œâ”€â”€ dim_users.sql             â­ Customer profiles
â”‚   â””â”€â”€ dim_user_segments.sql     â­ RFM segmentation
â””â”€â”€ facts/
    â”œâ”€â”€ fct_carts.sql             â­ Transaction facts
    â”œâ”€â”€ fct_daily_sales.sql       â­ Daily aggregates
    â””â”€â”€ fct_category_performance.sql â­ Category metrics
```

### Model Descriptions

#### 1. `dim_products` - Product Dimension

**Purpose**: Product catalog with enriched attributes
```sql
-- Business Logic:
-- 1. Extract products from raw.products
-- 2. Classify into tiers (Star, A, B, C) based on price
-- 3. Flag premium products (>$100)
-- 4. Clean and standardize names
```

**Key Fields**:
- `product_id`: Unique identifier
- `product_name`: Cleaned title
- `price`: Product price (USD)
- `category`: Product category
- `product_tier`: Star (>$200), A ($100-200), B ($50-100), C (<$50)
- `is_premium`: Boolean flag for >$100 products

**Tier Classification**:
```sql
CASE
    WHEN price >= 200 THEN 'Star'
    WHEN price >= 100 THEN 'A-Tier'
    WHEN price >= 50 THEN 'B-Tier'
    ELSE 'C-Tier'
END AS product_tier
```

**Tests**:
- âœ… `product_id` is unique
- âœ… `product_id` is not null
- âœ… `product_name` is not null
- âœ… `price` is not null
- âœ… `category` is not null

---

#### 2. `dim_users` - Customer Dimension

**Purpose**: Customer master data with cleaned attributes
```sql
-- Business Logic:
-- 1. Extract users from raw.users
-- 2. Combine first and last names
-- 3. Extract city from nested address JSON
-- 4. Standardize phone numbers
```

**Key Fields**:
- `user_id`: Unique customer identifier
- `email`: Customer email
- `full_name`: Concatenated firstname + lastname
- `city`: Extracted from address JSON
- `phone`: Contact number

**Name Concatenation**:
```sql
firstname || ' ' || lastname AS full_name
```

**Address Parsing**:
```sql
address::jsonb->>'city' AS city
```

**Tests**:
- âœ… `user_id` is unique
- âœ… `user_id` is not null
- âœ… `email` is unique
- âœ… `email` is not null
- âœ… `full_name` is not null

---

#### 3. `dim_user_segments` - Customer Segmentation (â­ Star Feature)

**Purpose**: RFM analysis for customer segmentation
```sql
-- Business Logic:
-- 1. Calculate Recency (days since last purchase)
-- 2. Calculate Frequency (total number of carts)
-- 3. Calculate Monetary (average cart value - not available in this dataset, using frequency as proxy)
-- 4. Score each dimension (1-5 scale)
-- 5. Assign segment based on combined RFM scores
```

**RFM Scoring**:

| Dimension | Calculation | Score Range |
|-----------|------------|-------------|
| **Recency** | Days since last cart | 1 (old) to 5 (recent) |
| **Frequency** | Total number of carts | 1 (few) to 5 (many) |
| **Monetary** | Proxy: frequency score | 1 (low) to 5 (high) |

**Segment Assignment**:
```sql
CASE
    WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'
    WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'
    WHEN recency_score >= 4 AND frequency_score <= 2 THEN 'New Customers'
    WHEN recency_score >= 3 AND frequency_score <= 2 THEN 'Promising'
    WHEN recency_score <= 2 AND frequency_score >= 4 THEN 'At Risk'
    WHEN recency_score <= 2 AND frequency_score >= 3 THEN 'Cant Lose Them'
    WHEN recency_score <= 2 AND frequency_score <= 2 THEN 'Hibernating'
    ELSE 'Lost'
END AS user_segment
```

**Segment Tiers**:
- **Tier 1**: Champions, Loyal Customers (VIP treatment)
- **Tier 2**: Potential Loyalists, New Customers (Growth opportunities)
- **Tier 3**: Promising, At Risk (Re-engagement needed)
- **Tier 4**: Can't Lose Them, Hibernating, Lost (Recovery campaigns)

**Tests**:
- âœ… `user_id` is unique
- âœ… `user_id` is not null
- âœ… `user_segment` is not null
- âœ… `user_segment` in accepted values (9 segments)
- âœ… Relationship: `user_id` exists in `dim_users`

---

#### 4. `fct_carts` - Transaction Facts

**Purpose**: Granular cart-level transactions
```sql
-- Business Logic:
-- 1. Extract carts from raw.carts
-- 2. Parse JSON products array
-- 3. Count total items and unique products
-- 4. Join with users for customer info
```

**Key Fields**:
- `cart_id`: Unique transaction identifier
- `user_id`: Foreign key to dim_users
- `cart_date`: Transaction timestamp
- `total_items`: Sum of all product quantities
- `unique_products`: Count of distinct products

**JSON Parsing**:
```sql
-- Extract products array
json_array_elements(products::json) AS product

-- Sum quantities
SUM((product->>'quantity')::int) AS total_items

-- Count unique products
COUNT(DISTINCT product->>'productId') AS unique_products
```

**Tests**:
- âœ… `cart_id` is unique
- âœ… `cart_id` is not null
- âœ… `user_id` is not null
- âœ… `total_items` is not null
- âœ… Relationship: `user_id` exists in `dim_users`

---

#### 5. `fct_daily_sales` - Daily Aggregates (Incremental)

**Purpose**: Daily sales metrics for trend analysis
```sql
-- Business Logic:
-- 1. Aggregate carts by date
-- 2. Calculate daily KPIs
-- 3. Incremental loading (only new dates)
```

**Materialization**: `incremental` (only processes new data)

**Key Fields**:
- `sale_date`: Date (grain of table)
- `total_carts`: Count of carts
- `unique_customers`: Count of distinct users
- `total_items_sold`: Sum of all items
- `avg_items_per_cart`: Average basket size
- `avg_carts_per_customer`: Purchase frequency

**Incremental Logic**:
```sql
{% if is_incremental() %}
    WHERE DATE(cart_date) > (SELECT MAX(sale_date) FROM {{ this }})
{% endif %}
```

**KPI Calculations**:
```sql
-- Average basket size
ROUND(total_items_sold::NUMERIC / NULLIF(total_carts, 0), 2) AS avg_items_per_cart

-- Purchase frequency
ROUND(total_carts::NUMERIC / NULLIF(unique_customers, 0), 2) AS avg_carts_per_customer
```

**Tests**:
- âœ… `sale_date` is unique
- âœ… `sale_date` is not null
- âœ… `total_carts` is not null
- âœ… `unique_customers` is not null

---

#### 6. `fct_category_performance` - Category Analytics

**Purpose**: Product category performance metrics
```sql
-- Business Logic:
-- 1. Group products by category
-- 2. Calculate aggregate statistics
-- 3. Identify pricing ranges
```

**Key Fields**:
- `category`: Product category (grain of table)
- `total_products`: Count of products
- `avg_price`: Average product price
- `min_price`: Lowest price in category
- `max_price`: Highest price in category
- `price_range`: Spread of prices

**Calculations**:
```sql
-- Price range
max_price - min_price AS price_range

-- Category with most products
RANK() OVER (ORDER BY COUNT(*) DESC) AS category_rank
```

**Tests**:
- âœ… `category` is unique
- âœ… `category` is not null
- âœ… `total_products` is not null
- âœ… `avg_price` is not null

---

### dbt Macros

**File**: `dbt-project/macros/`

#### 1. `generate_timestamp_column()`

Adds audit timestamp to all models:
```sql
{% macro generate_timestamp_column() %}
    CURRENT_TIMESTAMP as created_at
{% endmacro %}
```

#### 2. `calculate_rfm_score()`

RFM scoring logic (1-5 scale):
```sql
{% macro calculate_rfm_score(column_name, asc=true) %}
    NTILE(5) OVER (ORDER BY {{ column_name }} {{ 'ASC' if asc else 'DESC' }})
{% endmacro %}
```

Usage:
```sql
{{ calculate_rfm_score('days_since_last_purchase', asc=false) }} AS recency_score
```

#### 3. `cents_to_dollars()`

Currency conversion:
```sql
{% macro cents_to_dollars(column_name) %}
    ROUND({{ column_name }}::NUMERIC / 100, 2)
{% endmacro %}
```

#### 4. `generate_audit_columns()`

Full audit trail:
```sql
{% macro generate_audit_columns() %}
    CURRENT_TIMESTAMP as created_at,
    CURRENT_TIMESTAMP as updated_at,
    '{{ var("pipeline_version", "v1.0") }}' as pipeline_version
{% endmacro %}
```

---

## ğŸ“– Viewing dbt Documentation

dbt automatically generates beautiful, interactive documentation with data lineage.

### Method 1: Local dbt Docs Server
```bash
# Generate documentation
./scripts/run-dbt.sh "docs generate"

# Serve locally (opens browser automatically)
./scripts/run-dbt.sh "docs serve"

# Access at: http://localhost:8080
```
---

##  Sample Queries

### Query 1: Customer Segmentation Overview

**Business Question**: How are our customers distributed across segments?
```sql
-- Connect to database
kubectl exec -n ecommerce-pipeline \
  $(kubectl get pod -n ecommerce-pipeline -l app=postgresql -o jsonpath="{.items[0].metadata.name}") \
  -- psql -U ecommerceuser -d ecommerce_dw

-- Run query
SELECT 
    user_segment,
    segment_tier,
    COUNT(*) as customer_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentage,
    ROUND(AVG(recency_score), 2) as avg_recency,
    ROUND(AVG(frequency_score), 2) as avg_frequency
FROM analytics.dim_user_segments
GROUP BY user_segment, segment_tier
ORDER BY segment_tier, customer_count DESC;
```

**Expected Output**:
```
![Segements](images/Segmentation.png)
```

**Business Insights**:
- 50% of customers are Tier 1 (Champions + Loyal) â†’ Focus on retention
- 20% are "At Risk" â†’ Immediate win-back campaign needed
- 10% are "Hibernating" â†’ Consider re-engagement or archive

---

### Query 2: Top Performing Products

**Business Question**: Which products drive the most value?
```sql
SELECT 
    p.product_name,
    p.category,
    p.price,
    p.product_tier,
    p.is_premium,
    COUNT(*) as times_purchased
FROM analytics.dim_products p
JOIN raw.carts c ON TRUE  -- Join logic would use product_id in real scenario
ORDER BY p.price DESC, p.product_name
LIMIT 10;
```

**Expected Output**:
```
           product_name            |    category    | price  | product_tier | is_premium
-----------------------------------+----------------+--------+--------------+------------
 WD 2TB Elements Portable Drive   | electronics    | 64.00  | B-Tier       | f
 SanDisk SSD PLUS 1TB             | electronics    | 109.00 | A-Tier       | t
 Mens Cotton Jacket               | men's clothing | 55.99  | B-Tier       | f
```

**Business Insights**:
- Electronics dominate high-value purchases
- B-Tier products have high purchase frequency (sweet spot pricing)
- A-Tier products drive premium revenue

---

### Query 3: Daily Sales Trends

**Business Question**: How are sales trending over time?
```sql
SELECT 
    sale_date,
    total_carts,
    unique_customers,
    total_items_sold,
    avg_items_per_cart,
    avg_carts_per_customer,
    -- Calculate day-over-day growth
    LAG(total_carts) OVER (ORDER BY sale_date) as prev_day_carts,
    ROUND(
        (total_carts - LAG(total_carts) OVER (ORDER BY sale_date)) * 100.0 / 
        NULLIF(LAG(total_carts) OVER (ORDER BY sale_date), 0), 
        2
    ) as growth_rate_pct
FROM analytics.fct_daily_sales
ORDER BY sale_date DESC
LIMIT 30;
```

**Expected Output**:
```
 sale_date  | total_carts | unique_customers | total_items_sold | avg_items_per_cart | growth_rate_pct
------------+-------------+------------------+------------------+--------------------+-----------------
 2020-03-01 |          15 |               10 |              120 |               8.00 |           25.00
 2020-02-29 |          12 |                8 |               96 |               8.00 |          -14.29
 2020-02-28 |          14 |                9 |              112 |               8.00 |           16.67
```

**Business Insights**:
- Identify growth days vs. decline days
- Average basket size consistency (8 items/cart)
- Customer purchase frequency patterns

---

### Query 4: Category Performance Comparison

**Business Question**: Which product categories perform best?
```sql
SELECT 
    category,
    total_products,
    avg_price,
    min_price,
    max_price,
    price_range,
    -- Calculate category share of catalog
    ROUND(total_products * 100.0 / SUM(total_products) OVER (), 2) as catalog_share_pct,
    -- Rank categories
    RANK() OVER (ORDER BY avg_price DESC) as price_rank,
    RANK() OVER (ORDER BY total_products DESC) as volume_rank
FROM analytics.fct_category_performance
ORDER BY avg_price DESC;
```

**Expected Output**:
```
    category     | total_products | avg_price | min_price | max_price | price_range | catalog_share_pct
-----------------+----------------+-----------+-----------+-----------+-------------+-------------------
 electronics     |              6 |    500.00 |    100.00 |   1000.00 |      900.00 |             30.00
 jewelery        |              4 |    200.00 |     50.00 |    400.00 |      350.00 |             20.00
 men's clothing  |              5 |    100.00 |     20.00 |    200.00 |      180.00 |             25.00
 women's clothing|              5 |     80.00 |     15.00 |    150.00 |      135.00 |             25.00
```
---

## ğŸ“ Project Structure
```
ecommerce-customer-intelligence/
â”œâ”€â”€ .github/                          # GitHub Actions (optional)
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ pipeline.yml
â”œâ”€â”€ .gitlab-ci.yml                    # GitLab CI/CD pipeline â­
â”œâ”€â”€ .gitignore                        # Git ignore rules
â”œâ”€â”€ README.md                         # This file ğŸ“–
â”‚
â”œâ”€â”€ data-export/                      # Python extraction app
â”‚   â”œâ”€â”€ Dockerfile                    # Container image definition
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ main.py                       # ETL logic
â”‚
â”œâ”€â”€ dbt-project/                      # dbt transformations â­
â”‚   â”œâ”€â”€ Dockerfile                    # dbt container image
â”‚   â”œâ”€â”€ dbt_project.yml              # dbt project config
â”‚   â”œâ”€â”€ profiles.yml                 # Database connections
â”‚   â”œâ”€â”€ macros/                      # Reusable SQL functions
â”‚   â”‚   â”œâ”€â”€ generate_timestamp_column.sql
â”‚   â”‚   â”œâ”€â”€ calculate_rfm_score.sql
â”‚   â”‚   â”œâ”€â”€ cents_to_dollars.sql
â”‚   â”‚   â””â”€â”€ generate_audit_columns.sql
â”‚   â”œâ”€â”€ models/                      # SQL transformations
â”‚   â”‚   â”œâ”€â”€ dim_products.sql        # Product dimension
â”‚   â”‚   â”œâ”€â”€ dim_users.sql           # Customer dimension
â”‚   â”‚   â”œâ”€â”€ dim_user_segments.sql   # RFM segmentation â­
â”‚   â”‚   â”œâ”€â”€ fct_carts.sql           # Transaction facts
â”‚   â”‚   â”œâ”€â”€ fct_daily_sales.sql     # Daily aggregates
â”‚   â”‚   â”œâ”€â”€ fct_category_performance.sql
â”‚   â”‚   â”œâ”€â”€ schema.yml              # Model tests & docs
â”‚   â”‚   â””â”€â”€ sources.yml             # Source definitions
â”‚   â”œâ”€â”€ target/                      # Generated artifacts (gitignored)
â”‚   â””â”€â”€ logs/                        # dbt logs (gitignored)
â”‚
â”œâ”€â”€ kubernetes/                       # K8s manifests
â”‚   â”œâ”€â”€ namespace.yaml               # Namespace definition
â”‚   â”œâ”€â”€ postgresql.yaml              # PostgreSQL deployment
â”‚   â”œâ”€â”€ minio.yaml                   # MinIO deployment
â”‚   â””â”€â”€ airflow.yaml                 # Airflow (optional)
â”‚
â””â”€â”€ scripts/                         # Helper scripts
    â”œâ”€â”€ setup.sh                     # Full infrastructure setup
    â”œâ”€â”€ load-data.sh                 # Data extraction
    â”œâ”€â”€ run-dbt.sh                   # dbt wrapper
    â”œâ”€â”€ run-complete-workflow.sh     # End-to-end pipeline â­
    â””â”€â”€ run-tests.sh                 # Test execution
```

---


## ğŸ“Š Project Metrics

| Metric | Value |
|--------|-------|
| **dbt Models** | 6 (3 dimensions + 3 facts) |
| **Automated Tests** | 26 |
| **Customer Segments** | 9 (RFM-based) |
| **Data Sources** | 3 (products, users, carts) |
| **CI/CD Stages** | 4 (build, test, deploy, transform) |
| **Test Coverage** | 100% |

---

** To execute manually **
```bash
# Start your journey
./scripts/setup.sh
./scripts/run-complete-workflow.sh
```